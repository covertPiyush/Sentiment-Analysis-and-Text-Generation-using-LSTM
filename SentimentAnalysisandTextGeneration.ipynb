{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWWt1ezJp3af"
   },
   "source": [
    "INFO 6105\n",
    "\n",
    "Piyush Prashant\n",
    "\n",
    "NUID: 001444377\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbcojRY58T7I"
   },
   "source": [
    "\n",
    "  # Twitter Sentiment Analysis and Text Generation Using LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIB7ecZvtkmt"
   },
   "source": [
    "### What are we trying to do?\n",
    "In this blog, I am trying to analyze social media  contents related to Artificial Intelligence and the Data sciences and do a sentimental analysis and do generaative language Modeling Based on the sentiment classification I am selecting the dataset and using it to train are generative model. I am using Twitter tweets to do the analysis. Later on in the future  I would like to generate my  content based on the analysis as well as some performance metric to choose the tweets and post them for the AI skunkworks community LinkedIn Page.\n",
    "\n",
    "### Natural Language Processing(NLP)\n",
    "Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "### Natural Language Generation(NLG)\n",
    "Natural-language generation (NLG) is one of the tasks of natural language processing that focuses on generating natural language from structured data such as a knowledge base or a logical form (linguistics). \n",
    "\n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "Recurrent Neural Networks (RNNs) are popular models that are greatly used in many Natural Language Processing (NLP) tasks. A recurrent neural network (RNN) is a part of artificial neural network (ANN) where connections between units form a directed graph along a sequence. This allows it to exhibit dynamic temporal behavior for a time sequence. RNNs use their internal state to process the sequence of inputs. This is not possible in feedforward neural networks. They are neural networks which repeatedly make use of sequential information.The main assumption in a traditional artificial neural network is that all inputs and outputs are independent of each other and are not in sequence. But if we look at it, it is not such a great way to approach ANNs. Imagine a scenario where you have to predict the next song a user might want to listen. This will be impossible if the model won't know what songs have already been played. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being dependent on the previous computations.\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/afqRj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DHa_zNpKd59"
   },
   "source": [
    "### LSTM\n",
    "Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies.They  are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn. All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ImIVlFrMhu3"
   },
   "source": [
    "In this project, we are trying to generate the next text that the machine will predict according to the inputs we provide. We are using Artificial neural networks instead of staistical models as we try to guess a human's behavior. The complexity of training an ANN is higher as compared to statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQcclXjsMrs5"
   },
   "source": [
    "#### Importing necessary dependencies like textblob, tensorflow and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYoWhLr6gSUr"
   },
   "outputs": [],
   "source": [
    "#import pickle as pkl\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import  TextBlob\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "import numpy as np\n",
    "import string, os \n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9933,
     "status": "ok",
     "timestamp": 1556311642355,
     "user": {
      "displayName": "Piyush Lnu",
      "photoUrl": "",
      "userId": "09243715371298254396"
     },
     "user_tz": 240
    },
    "id": "VkJcv_S3Hrh0",
    "outputId": "c5c2c425-01f5-4ef4-9595-0317dd27ea08"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-f55f7c45-a442-48a7-8233-50945c779e42\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-f55f7c45-a442-48a7-8233-50945c779e42\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving AIDatascienceTwitter.csv to AIDatascienceTwitter (4).csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_v2U3z5NJtu"
   },
   "source": [
    "We preprocess the data and standardize the data by converting the text into lower caase as our model might treat a word which is in the beginning of a sentence with a capital letter different from the same word which appears later in the sentence but without any capital latter. This might lead to decline in the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwgBKsgLo90m"
   },
   "outputs": [],
   "source": [
    "# Cleaning the tweets\n",
    "def clean_tweet(tweet):\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    tweet = \"\".join(v for v in tweet if v not in string.punctuation).lower()\n",
    "    tweet = tweet.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACdb-j1rP1r3"
   },
   "source": [
    "Here we create a DataFrame and drop the records with empty tweets\n",
    "\n",
    "The data here is scrapped from twitter using Octoparse tool. I have scrapped around 1000 tweets related to Artificial Intelligence and Datascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1556311656513,
     "user": {
      "displayName": "Piyush Lnu",
      "photoUrl": "",
      "userId": "09243715371298254396"
     },
     "user_tz": 240
    },
    "id": "G1hcsBtNSnnH",
    "outputId": "8344e926-2142-4c2f-db95-20e233c5d831"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>big data can help us see through government re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blending artificial intelligence and human cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data science is constantly evolving as technol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data science is constantly evolving as technol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ai languagelearning datascience artificialinte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet\n",
       "0  big data can help us see through government re...\n",
       "1  blending artificial intelligence and human cre...\n",
       "2  data science is constantly evolving as technol...\n",
       "3  data science is constantly evolving as technol...\n",
       "4  ai languagelearning datascience artificialinte..."
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataProcessing\n",
    "Data = pd.read_csv('AIDatascienceTwitter.csv', usecols =['Tweet'])\n",
    "Data = Data.dropna()\n",
    "modData =  pd.DataFrame(Data['Tweet'].apply(clean_tweet))\n",
    "modData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-H8sJJLvQIn-"
   },
   "source": [
    "We create function that uses the textblob library and classify the sentiments of the tweets by checking its polarity. Negative polarity corresponds to negative sentiments and vice versa for positive sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERsZKGrfSq0H"
   },
   "outputs": [],
   "source": [
    "#Getting tweet Sentiment\n",
    "def get_tweet_sentiment(tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(tweet) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5iETsO9SwoB"
   },
   "outputs": [],
   "source": [
    "# Empty list to store tweets\n",
    "tweets = [] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jhAkSZ0RrHC"
   },
   "source": [
    "A function is created to store the tweets and their respective sentiments in dictionary and the dictionary object is stored in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HF8omcgKS-50"
   },
   "outputs": [],
   "source": [
    "for tweet in modData['Tweet']:\n",
    "#         print(tweet)\n",
    "         # empty dictionary to store required params of a tweet \n",
    "         parsed_tweet = {} \n",
    "         parsed_tweet['text'] = tweet\n",
    "         parsed_tweet['sentiment'] = get_tweet_sentiment(tweet) \n",
    "         tweets.append(parsed_tweet) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y67ehgJ_sRx_"
   },
   "source": [
    "The analysed tweets are calculated for their share in the sentiment spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1556311685087,
     "user": {
      "displayName": "Piyush Lnu",
      "photoUrl": "",
      "userId": "09243715371298254396"
     },
     "user_tz": 240
    },
    "id": "FFIIXDPXZmP_",
    "outputId": "191064ce-b3c2-4dd2-87a4-9ff94126eaba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets percentage: 45.19131334022751 %\n",
      "Negative tweets percentage: 15.201654601861428 %\n",
      "Neutral tweets percentage: 39.60703205791106 %\n"
     ]
    }
   ],
   "source": [
    "ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "# percentage of positive tweets \n",
    "print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "# picking negative tweets from tweets \n",
    "ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "# percentage of negative tweets \n",
    "print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "# percentage of neutral tweets \n",
    "print(\"Neutral tweets percentage: {} %\".format(100*(len(tweets) - len(ntweets) - len(ptweets))/len(tweets))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1556311687887,
     "user": {
      "displayName": "Piyush Lnu",
      "photoUrl": "",
      "userId": "09243715371298254396"
     },
     "user_tz": 240
    },
    "id": "ckr_ZmElSrG2",
    "outputId": "3c934f56-f0c4-4533-ecee-1e4b51c11fd1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data to plot\n",
    "labels = 'Positive', 'Negative', 'Neutral'\n",
    "sizes = [len(ptweets), len(ntweets), (len(tweets) - len(ntweets) - len(ptweets))]\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral']\n",
    "explode = (0.1, 0,0)  # explode 1st slice\n",
    " \n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,autopct='%1.1f%%', shadow=True, startangle=140)\n",
    " \n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LhTHJpZ4sc5f"
   },
   "source": [
    "Here we want to to use only those tweets which have positive or neutral  sentiments and use them to train and generate new tweets for our purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOnTf0ZZZwNJ"
   },
   "outputs": [],
   "source": [
    "# Positvie and neutral tweets are stored in a list \n",
    "corpus = [tweet['text'] for tweet in tweets if tweet['sentiment'] == 'positive' or tweet['sentiment'] == 'neutral']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5uU2tECG1GFx"
   },
   "source": [
    "#### Generating Sequence of N-gram Tokens\n",
    "Language modelling requires a sequence input data, as given a sequence (of words/tokens) the aim is the predict next word/token.\n",
    "\n",
    "\n",
    "The next step is Tokenization. It is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded. The tokens become the input for another process like parsing and text mining.\n",
    "\n",
    "\n",
    "\n",
    "Python’s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgvnYEMdZyh5"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeCVzAovZ3eM"
   },
   "outputs": [],
   "source": [
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1556311703472,
     "user": {
      "displayName": "Piyush Lnu",
      "photoUrl": "",
      "userId": "09243715371298254396"
     },
     "user_tz": 240
    },
    "id": "vvSMZuW-7Vt0",
    "outputId": "f804b3a1-05ff-43e0-e628-e37ee5ccb9bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 3640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[53, 12],\n",
       " [53, 12, 91],\n",
       " [53, 12, 91, 139],\n",
       " [53, 12, 91, 139, 239],\n",
       " [53, 12, 91, 139, 239, 130],\n",
       " [53, 12, 91, 139, 239, 130, 240],\n",
       " [53, 12, 91, 139, 239, 130, 240, 542],\n",
       " [53, 12, 91, 139, 239, 130, 240, 542, 943],\n",
       " [53, 12, 91, 139, 239, 130, 240, 542, 943, 543],\n",
       " [53, 12, 91, 139, 239, 130, 240, 542, 943, 543, 22]]"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "print('Total words:' , total_words)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ijdyRBa7WsQ"
   },
   "source": [
    "In the above output [53, 12], [53, 12, 91],  [53, 12, 91, 139] and so on represents the ngram phrases generated from the input data. where every integer corresponds to the index of a particular word in the complete vocabulary of words present in the text. For example\n",
    "\n",
    "#### Headline: i stand with the shedevils\n",
    "#### Ngrams: | Sequence of Tokens\n",
    "\n",
    "\n",
    "|Ngram  |Sequence of Tokens   |\n",
    "|------|------|\n",
    "|   i stand  |  [53, 12]|\n",
    "|i stand with| [53, 12, 91] |\n",
    "|i stand with th|[53, 12, 91, 139]|\n",
    "|i stand with the shedevils|[53, 12, 91, 139, 239]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8JEFOmJB6WF"
   },
   "source": [
    "#### Padding the Sequences and obtain Variables : Predictors and Target\n",
    "Now that we have generated a data-set which contains sequence of tokens, it is possible that different sequences have different lengths. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Kears for this purpose. To input this data into a learning model, we need to create predictors and label. We will create N-grams sequence as predictors and the next word of the N-gram as label. For example:\n",
    "\n",
    "**Headline**: they are learning data science\n",
    "\n",
    "|PREDICTORS|LABEL|\n",
    "|----|----|\n",
    "|they|are|\n",
    "|they are|learning|\n",
    "|they are learning|they are learning|\n",
    "|they are learning data|science|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21RvO4NgZ7ed"
   },
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVqyScQZ93d"
   },
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KV0y-EU5DPMK"
   },
   "source": [
    "Now we can obtain the input vector X and the label vector Y which can be used for the training purposes. Recent experiments have shown that recurrent neural networks have shown a good performance in sequence to sequence learning and text data applications. Lets look at them in brief.\n",
    "\n",
    "![alt text](http://www.shivambansal.com/blog/text-lstm/2.png)\n",
    "\n",
    "\n",
    "Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a ‘memory state’ of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n",
    "\n",
    "The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n",
    "\n",
    "LSTMs have an additional state called ‘cell state’ through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively.  Lets architecture a LSTM model in our code. I have added total three layers in the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.   Input Layer : Takes the sequence of words as input\n",
    "2.   LSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.\n",
    "3. Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting. (Optional Layer)\n",
    "4. Output Layer : Computes the probability of the best possible next word as output\n",
    "\n",
    "\n",
    "\n",
    "We will run this model for total 100 epoochs but it can be experimented further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BovaYJ8AZ_vV"
   },
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1556311845021,
     "user": {
      "displayName": "Piyush Lnu",
      "photoUrl": "",
      "userId": "09243715371298254396"
     },
     "user_tz": 240
    },
    "id": "rPyYdEZfaCK4",
    "outputId": "403382dd-0d2e-4439-e450-e6f39c23bf9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 45, 10)            36400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3640)              367640    \n",
      "=================================================================\n",
      "Total params: 448,440\n",
      "Trainable params: 448,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alwu92mOWDL9"
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1053528,
     "status": "ok",
     "timestamp": 1556315769626,
     "user": {
      "displayName": "Piyush Lnu",
      "photoUrl": "",
      "userId": "09243715371298254396"
     },
     "user_tz": 240
    },
    "id": "VVhTsfZdarWd",
    "outputId": "18130209-2031-4fa0-d372-fb8bfb96bf2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.21582, saving model to weights-improvement-01-6.2158.hdf5\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: loss improved from 6.21582 to 5.78879, saving model to weights-improvement-02-5.7888.hdf5\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: loss improved from 5.78879 to 5.50650, saving model to weights-improvement-03-5.5065.hdf5\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: loss improved from 5.50650 to 5.23717, saving model to weights-improvement-04-5.2372.hdf5\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: loss improved from 5.23717 to 4.97961, saving model to weights-improvement-05-4.9796.hdf5\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: loss improved from 4.97961 to 4.74217, saving model to weights-improvement-06-4.7422.hdf5\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: loss improved from 4.74217 to 4.52269, saving model to weights-improvement-07-4.5227.hdf5\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: loss improved from 4.52269 to 4.32444, saving model to weights-improvement-08-4.3244.hdf5\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: loss improved from 4.32444 to 4.13058, saving model to weights-improvement-09-4.1306.hdf5\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: loss improved from 4.13058 to 3.95435, saving model to weights-improvement-10-3.9544.hdf5\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: loss improved from 3.95435 to 3.77810, saving model to weights-improvement-11-3.7781.hdf5\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: loss improved from 3.77810 to 3.62344, saving model to weights-improvement-12-3.6234.hdf5\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: loss improved from 3.62344 to 3.46622, saving model to weights-improvement-13-3.4662.hdf5\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: loss improved from 3.46622 to 3.31809, saving model to weights-improvement-14-3.3181.hdf5\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: loss improved from 3.31809 to 3.17825, saving model to weights-improvement-15-3.1783.hdf5\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: loss improved from 3.17825 to 3.04536, saving model to weights-improvement-16-3.0454.hdf5\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: loss improved from 3.04536 to 2.92205, saving model to weights-improvement-17-2.9221.hdf5\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: loss improved from 2.92205 to 2.80219, saving model to weights-improvement-18-2.8022.hdf5\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: loss improved from 2.80219 to 2.68943, saving model to weights-improvement-19-2.6894.hdf5\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: loss improved from 2.68943 to 2.58648, saving model to weights-improvement-20-2.5865.hdf5\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: loss improved from 2.58648 to 2.48740, saving model to weights-improvement-21-2.4874.hdf5\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: loss improved from 2.48740 to 2.40027, saving model to weights-improvement-22-2.4003.hdf5\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: loss improved from 2.40027 to 2.31250, saving model to weights-improvement-23-2.3125.hdf5\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: loss improved from 2.31250 to 2.23273, saving model to weights-improvement-24-2.2327.hdf5\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: loss improved from 2.23273 to 2.15295, saving model to weights-improvement-25-2.1529.hdf5\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: loss improved from 2.15295 to 2.08410, saving model to weights-improvement-26-2.0841.hdf5\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: loss improved from 2.08410 to 2.02055, saving model to weights-improvement-27-2.0206.hdf5\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: loss improved from 2.02055 to 1.96540, saving model to weights-improvement-28-1.9654.hdf5\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: loss improved from 1.96540 to 1.90019, saving model to weights-improvement-29-1.9002.hdf5\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: loss improved from 1.90019 to 1.84502, saving model to weights-improvement-30-1.8450.hdf5\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: loss improved from 1.84502 to 1.79292, saving model to weights-improvement-31-1.7929.hdf5\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: loss improved from 1.79292 to 1.74049, saving model to weights-improvement-32-1.7405.hdf5\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: loss improved from 1.74049 to 1.69409, saving model to weights-improvement-33-1.6941.hdf5\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: loss improved from 1.69409 to 1.64963, saving model to weights-improvement-34-1.6496.hdf5\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: loss improved from 1.64963 to 1.60782, saving model to weights-improvement-35-1.6078.hdf5\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: loss improved from 1.60782 to 1.57378, saving model to weights-improvement-36-1.5738.hdf5\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: loss improved from 1.57378 to 1.52700, saving model to weights-improvement-37-1.5270.hdf5\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: loss improved from 1.52700 to 1.49497, saving model to weights-improvement-38-1.4950.hdf5\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: loss improved from 1.49497 to 1.44803, saving model to weights-improvement-39-1.4480.hdf5\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: loss improved from 1.44803 to 1.41891, saving model to weights-improvement-40-1.4189.hdf5\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: loss improved from 1.41891 to 1.39010, saving model to weights-improvement-41-1.3901.hdf5\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: loss improved from 1.39010 to 1.35734, saving model to weights-improvement-42-1.3573.hdf5\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: loss improved from 1.35734 to 1.32506, saving model to weights-improvement-43-1.3251.hdf5\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: loss improved from 1.32506 to 1.29400, saving model to weights-improvement-44-1.2940.hdf5\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: loss improved from 1.29400 to 1.26971, saving model to weights-improvement-45-1.2697.hdf5\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: loss improved from 1.26971 to 1.24049, saving model to weights-improvement-46-1.2405.hdf5\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: loss improved from 1.24049 to 1.21492, saving model to weights-improvement-47-1.2149.hdf5\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 00048: loss improved from 1.21492 to 1.19074, saving model to weights-improvement-48-1.1907.hdf5\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: loss improved from 1.19074 to 1.16458, saving model to weights-improvement-49-1.1646.hdf5\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: loss improved from 1.16458 to 1.13600, saving model to weights-improvement-50-1.1360.hdf5\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: loss improved from 1.13600 to 1.11179, saving model to weights-improvement-51-1.1118.hdf5\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: loss improved from 1.11179 to 1.10442, saving model to weights-improvement-52-1.1044.hdf5\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 00053: loss improved from 1.10442 to 1.07623, saving model to weights-improvement-53-1.0762.hdf5\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: loss improved from 1.07623 to 1.05265, saving model to weights-improvement-54-1.0527.hdf5\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 00055: loss improved from 1.05265 to 1.02548, saving model to weights-improvement-55-1.0255.hdf5\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 00056: loss improved from 1.02548 to 1.01886, saving model to weights-improvement-56-1.0189.hdf5\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: loss improved from 1.01886 to 0.99387, saving model to weights-improvement-57-0.9939.hdf5\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: loss improved from 0.99387 to 0.97531, saving model to weights-improvement-58-0.9753.hdf5\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 00059: loss improved from 0.97531 to 0.95352, saving model to weights-improvement-59-0.9535.hdf5\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 00060: loss improved from 0.95352 to 0.95290, saving model to weights-improvement-60-0.9529.hdf5\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: loss improved from 0.95290 to 0.92673, saving model to weights-improvement-61-0.9267.hdf5\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 00062: loss improved from 0.92673 to 0.91022, saving model to weights-improvement-62-0.9102.hdf5\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: loss improved from 0.91022 to 0.88728, saving model to weights-improvement-63-0.8873.hdf5\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: loss improved from 0.88728 to 0.87150, saving model to weights-improvement-64-0.8715.hdf5\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 00065: loss improved from 0.87150 to 0.86506, saving model to weights-improvement-65-0.8651.hdf5\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 00066: loss improved from 0.86506 to 0.85025, saving model to weights-improvement-66-0.8503.hdf5\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: loss improved from 0.85025 to 0.83319, saving model to weights-improvement-67-0.8332.hdf5\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: loss improved from 0.83319 to 0.82167, saving model to weights-improvement-68-0.8217.hdf5\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: loss improved from 0.82167 to 0.80608, saving model to weights-improvement-69-0.8061.hdf5\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 00070: loss improved from 0.80608 to 0.79202, saving model to weights-improvement-70-0.7920.hdf5\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: loss improved from 0.79202 to 0.78316, saving model to weights-improvement-71-0.7832.hdf5\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 00072: loss improved from 0.78316 to 0.76913, saving model to weights-improvement-72-0.7691.hdf5\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: loss improved from 0.76913 to 0.75288, saving model to weights-improvement-73-0.7529.hdf5\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: loss improved from 0.75288 to 0.75202, saving model to weights-improvement-74-0.7520.hdf5\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: loss improved from 0.75202 to 0.73697, saving model to weights-improvement-75-0.7370.hdf5\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 00076: loss improved from 0.73697 to 0.72183, saving model to weights-improvement-76-0.7218.hdf5\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: loss improved from 0.72183 to 0.71845, saving model to weights-improvement-77-0.7184.hdf5\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: loss improved from 0.71845 to 0.70585, saving model to weights-improvement-78-0.7059.hdf5\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 00079: loss improved from 0.70585 to 0.69921, saving model to weights-improvement-79-0.6992.hdf5\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 00080: loss improved from 0.69921 to 0.69113, saving model to weights-improvement-80-0.6911.hdf5\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: loss improved from 0.69113 to 0.67414, saving model to weights-improvement-81-0.6741.hdf5\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 00082: loss improved from 0.67414 to 0.65937, saving model to weights-improvement-82-0.6594.hdf5\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.65937\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 00084: loss improved from 0.65937 to 0.65545, saving model to weights-improvement-84-0.6554.hdf5\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: loss improved from 0.65545 to 0.64112, saving model to weights-improvement-85-0.6411.hdf5\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 00086: loss improved from 0.64112 to 0.62785, saving model to weights-improvement-86-0.6278.hdf5\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.62785\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: loss improved from 0.62785 to 0.61650, saving model to weights-improvement-88-0.6165.hdf5\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 00089: loss improved from 0.61650 to 0.61453, saving model to weights-improvement-89-0.6145.hdf5\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 00090: loss improved from 0.61453 to 0.61273, saving model to weights-improvement-90-0.6127.hdf5\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 00091: loss improved from 0.61273 to 0.59296, saving model to weights-improvement-91-0.5930.hdf5\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: loss improved from 0.59296 to 0.58442, saving model to weights-improvement-92-0.5844.hdf5\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 00093: loss improved from 0.58442 to 0.57145, saving model to weights-improvement-93-0.5715.hdf5\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.57145\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 00095: loss improved from 0.57145 to 0.56284, saving model to weights-improvement-95-0.5628.hdf5\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.56284\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: loss improved from 0.56284 to 0.55117, saving model to weights-improvement-97-0.5512.hdf5\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 00098: loss improved from 0.55117 to 0.54572, saving model to weights-improvement-98-0.5457.hdf5\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 00099: loss improved from 0.54572 to 0.54147, saving model to weights-improvement-99-0.5415.hdf5\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 00100: loss improved from 0.54147 to 0.53959, saving model to weights-improvement-100-0.5396.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc14e97048>"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=5, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmxazz6mLu25"
   },
   "source": [
    "### Generating the text\n",
    "Our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdPb3n4maHnW"
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1556319321345,
     "user": {
      "displayName": "Piyush Lnu",
      "photoUrl": "",
      "userId": "09243715371298254396"
     },
     "user_tz": 240
    },
    "id": "aumB3jL0nwx2",
    "outputId": "85e74dd3-a63b-42ee-e0eb-e9c99710cc06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data Analytics In Healthcare Market To Reach 34 16 Bn In\n",
      "Neural Nets Learning To Service April 22 In Seattle New Medium Ai\n",
      "Ai Tool Could Help Diagnose Alzheimer S Ai Ml Dl Machinelearning\n",
      "World To Use Model What S Not Big Data And The\n",
      "Machine Learning Algorithms In One Chart Via Ai Deeplearning Machinelearning Datascience Bigdatapic\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"big data\", 10, model, max_sequence_len))\n",
    "print (generate_text(\"Neural Nets\", 10, model, max_sequence_len))\n",
    "print (generate_text(\"AI\", 10, model, max_sequence_len))\n",
    "print (generate_text(\"World\", 10, model, max_sequence_len))\n",
    "print (generate_text(\"Machine Learning\", 10, model, max_sequence_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZGOHuJLqHPa"
   },
   "source": [
    "# Conclusion \n",
    "As we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n",
    "\n",
    "### Future Improvements\n",
    "   1. The selection data can be based on their performance in social media ,i.e. that is cllecting tweets  and posts which have high user acceptance. \n",
    "   2. We could increase the size of the dataset and scrap data across various social media platforms .\n",
    "   3. Fine Tuning the network architecture\n",
    "   4. Fine Tuning the network parameters\n",
    "   5. We could compare the LSTM against diffrent RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKZ_waJqKacn"
   },
   "source": [
    "## Contribution Stataement\n",
    "\n",
    "40% work carried out and contributed by Piyush Prashant\n",
    "\n",
    "60% - External resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ahxYyAjKodO"
   },
   "source": [
    "## Licenses\n",
    "\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Piyush Prashant\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieEN9dEdKt1m"
   },
   "source": [
    "## References\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/<br>\n",
    "https://www.datacamp.com/community/tutorials/deep-learning-python<br>\n",
    "https://github.com/tensorflow/models<br>\n",
    "https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/<br>\n",
    "https://en.wikipedia.org/wiki/Long_short-term_memory<br>\n",
    "https://deeplearning4j.org/lstm.html<br>\n",
    "https://keras.io/layers/core/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCrccjIZKyDx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
